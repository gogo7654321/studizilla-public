# AP Statistics Unit 2 Study Guide

:::GUIDE:::
unit::=Unit 2
title::=üìà Unit 2: Exploring Two-Variable Data Complete Guide
desc::=Master scatterplots, correlation, regression, and relationships between quantitative variables
diff::=Medium
time::=50 minutes
tags::=statistics, correlation, regression, scatterplots, two-variable, linear
content::=

# üìà Unit 2: Exploring Two-Variable Data

## üìã Unit Overview

How do two quantitative variables relate to each other? This unit explores scatterplots, correlation, and linear regression to understand and predict relationships! üìä

:::TIMELINE:::
id::=history-correlation-regression
title::=History of Correlation and Regression
events::=[
  {"year": "1805", "event": "Legendre's Least Squares", "detail": "Adrien-Marie Legendre published the method of least squares for fitting curves to data, though Gauss claimed to have used it earlier."},
  {"year": "1809", "event": "Gauss's Error Theory", "detail": "Carl Friedrich Gauss published work on the normal distribution and least squares method for astronomical calculations."},
  {"year": "1869", "event": "Galton Studies Heredity", "detail": "Francis Galton began studying heredity, comparing heights of parents and children, leading to the discovery of regression."},
  {"year": "1877", "event": "Regression to the Mean", "detail": "Galton coined 'regression to the mean' observing that tall parents tend to have children closer to average height."},
  {"year": "1886", "event": "Galton's Regression Line", "detail": "Galton published the first regression line relating heights of parents to children in his paper on hereditary stature."},
  {"year": "1888", "event": "Correlation Coefficient", "detail": "Galton introduced the concept of correlation (originally called 'co-relation') to measure the strength of relationships."},
  {"year": "1896", "event": "Pearson's Formula", "detail": "Karl Pearson developed the mathematical formula for the correlation coefficient (r) that we use today."},
  {"year": "1901", "event": "Biometrika Founded", "detail": "Pearson founded the journal Biometrika, establishing correlation and regression as fundamental statistical tools."},
  {"year": "1908", "event": "Student's t-Distribution", "detail": "William Gosset (Student) developed methods for inference with small samples, later applied to regression."},
  {"year": "1922", "event": "R.A. Fisher's Work", "detail": "Ronald Fisher formalized the mathematical theory of regression and introduced analysis of variance."}
]
:::/TIMELINE:::

### Essential Questions

| Question | Focus |
|----------|-------|
| How do we display two-variable data? | Scatterplots |
| How do we measure association? | Correlation coefficient |
| How do we model relationships? | Linear regression |
| How do we interpret regression? | Slope, intercept, r¬≤ |
| What are the dangers? | Extrapolation, lurking variables |

### Key Concepts

| Concept | Description |
|---------|-------------|
| **Scatterplot** | Graph of paired data |
| **Correlation (r)** | Strength and direction |
| **Regression line** | Prediction equation |
| **Residuals** | Observed - Predicted |
| **r¬≤** | Variation explained |

---

## üèÜ Famous Statisticians in Regression

| Statistician | Contribution | Era |
|--------------|--------------|-----|
| **Carl Friedrich Gauss** | Least squares method, normal distribution | 1800s |
| **Adrien-Marie Legendre** | Published least squares method | 1805 |
| **Francis Galton** | Discovered regression, coined correlation | 1880s |
| **Karl Pearson** | Correlation coefficient formula | 1896 |
| **Ronald Fisher** | Modern regression theory | 1920s |

---

## üìä Scatterplots

### Creating a Scatterplot

| Concept | Description |
|---------|-------------|
| **Explanatory variable (x)** | Independent, horizontal axis |
| **Response variable (y)** | Dependent, vertical axis |
| **Each point** | One (x, y) pair |

### Describing Scatterplots: DOFS

| Letter | Element | Description |
|--------|---------|-------------|
| **D** | Direction | Positive, negative, none |
| **O** | Outliers | Points away from pattern |
| **F** | Form | Linear, curved, clusters |
| **S** | Strength | Weak, moderate, strong |

### Direction

| Direction | Description |
|-----------|-------------|
| **Positive** | As x increases, y increases |
| **Negative** | As x increases, y decreases |
| **None** | No apparent pattern |

### Form

| Form | Description |
|------|-------------|
| **Linear** | Straight line pattern |
| **Curved** | Nonlinear relationship |
| **Clusters** | Groups of points |
| **No pattern** | Scattered randomly |

### Strength

| Strength | Description |
|----------|-------------|
| **Strong** | Points close to pattern |
| **Moderate** | Some scatter around pattern |
| **Weak** | Points far from pattern |

### Example Scatterplot Descriptions

| Scenario | DOFS Description |
|----------|------------------|
| Height vs. Weight | Positive, linear, moderate to strong, no obvious outliers |
| Age vs. Reaction Time | Positive, linear, moderate (older = slower), possible outliers among elderly |
| Study Hours vs. Test Score | Positive, linear, moderate, possible ceiling effect at high hours |
| Temperature vs. Ice Cream Sales | Positive, possibly curved (levels off at extremes), strong |
| Car Age vs. Price | Negative, curved (exponential decay), strong |

### Real-World Two-Variable Relationships

| Relationship | Direction | Form | Strength |
|--------------|-----------|------|----------|
| Education level vs. Income | Positive | Linear | Moderate |
| Speed vs. Fuel efficiency | Negative | Curved | Strong |
| Exercise vs. Resting heart rate | Negative | Linear | Moderate |
| Hours of sleep vs. Test performance | Positive | Linear (with threshold) | Moderate |
| Altitude vs. Temperature | Negative | Linear | Strong |

---

## üìè Correlation Coefficient (r)

### Definition

| Concept | Description |
|---------|-------------|
| **Symbol** | r |
| **Range** | -1 ‚â§ r ‚â§ 1 |
| **Measures** | Strength and direction of LINEAR relationship |

### Formula

| Formula | |
|---------|--|
| **r** | = (1/(n-1)) Œ£[(x - xÃÑ)/s‚Çì][(y - »≥)/s·µß] |
| **Or** | = Œ£z‚Çìz·µß / (n-1) |

### Interpreting r

| r Value | Interpretation |
|---------|----------------|
| **r = 1** | Perfect positive linear |
| **r = -1** | Perfect negative linear |
| **r = 0** | No linear relationship |
| **0.8 < |r| ‚â§ 1** | Strong |
| **0.5 < |r| ‚â§ 0.8** | Moderate |
| **|r| ‚â§ 0.5** | Weak |

### Properties of r

| Property | Explanation |
|----------|-------------|
| **No units** | Standardized |
| **Symmetric** | r(x,y) = r(y,x) |
| **Linear only** | Doesn't detect curved relationships |
| **Affected by outliers** | Not resistant |

### r Does NOT Tell You

| What r Misses | Example |
|---------------|---------|
| **Curved patterns** | Quadratic with r ‚âà 0 |
| **Appropriateness** | May be linear by chance |
| **Causation** | Correlation ‚â† causation |

### Visual Guide to Correlation Values

| r Value | Description | Visual Pattern |
|---------|-------------|----------------|
| r = 1.0 | Perfect positive | All points on upward line |
| r = 0.9 | Strong positive | Points tightly clustered, upward |
| r = 0.6 | Moderate positive | Points scattered, upward trend |
| r = 0.3 | Weak positive | Points very scattered, slight upward |
| r = 0.0 | No linear relationship | Random scatter |
| r = -0.5 | Moderate negative | Points scattered, downward trend |
| r = -1.0 | Perfect negative | All points on downward line |

### Common Mistakes with Correlation

| Mistake | Reality |
|---------|---------|
| "r = 0 means no relationship" | r = 0 means no LINEAR relationship |
| "High r proves causation" | Correlation never proves causation |
| "r measures any pattern" | r only measures linear patterns |
| "r can be greater than 1" | r is always between -1 and 1 |

---

## üìê Least Squares Regression Line (LSRL)

### Definition

| Concept | Description |
|---------|-------------|
| **LSRL** | Line that minimizes sum of squared residuals |
| **Equation** | ≈∑ = a + bx (or ≈∑ = b‚ÇÄ + b‚ÇÅx) |
| **≈∑** | Predicted y value |

### Calculating Slope (b)

| Formula | |
|---------|--|
| **b** | = r √ó (s·µß/s‚Çì) |

### Calculating Intercept (a)

| Formula | |
|---------|--|
| **a** | = »≥ - b √ó xÃÑ |

### The Line Always Passes Through

| Point | (xÃÑ, »≥) |
|-------|---------|
| **The means** | Center of the data |

### Complete Example: Finding the LSRL

**Data:** Hours studied (x) vs. Test Score (y)

| x (hours) | y (score) |
|-----------|-----------|
| 2 | 65 |
| 3 | 70 |
| 4 | 75 |
| 5 | 82 |
| 6 | 88 |

**Step 1:** Calculate summary statistics
- xÃÑ = 4, »≥ = 76
- s‚Çì = 1.58, s·µß = 9.08
- r = 0.995

**Step 2:** Calculate slope
b = r √ó (s·µß/s‚Çì) = 0.995 √ó (9.08/1.58) = 5.72

**Step 3:** Calculate intercept
a = »≥ - b √ó xÃÑ = 76 - 5.72 √ó 4 = 53.12

**LSRL:** ≈∑ = 53.12 + 5.72x

**Interpretation:** 
- Slope: For each additional hour of studying, we predict the test score to increase by 5.72 points
- Intercept: When study hours = 0, we predict a score of 53.12 (extrapolation - be cautious!)

---

## üìù Interpreting Regression

### Interpreting Slope

| Template | |
|----------|--|
| **Say** | "For each additional [unit of x], we predict [y] to [increase/decrease] by [b] [units of y]." |
| **Example** | "For each additional year of education, we predict income to increase by $5,000." |

### Interpreting Intercept

| Template | |
|----------|--|
| **Say** | "When [x] = 0, we predict [y] to be [a] [units]." |
| **Caution** | Often meaningless (extrapolation) |
| **Example** | "When study hours = 0, we predict a score of 45." |

### When Intercept Makes Sense

| If | Then |
|----|------|
| **x = 0 is in data range** | Intercept is meaningful |
| **x = 0 is outside range** | Intercept is extrapolation |

---

## üìä Residuals

### Definition

| Concept | Description |
|---------|-------------|
| **Residual** | Observed - Predicted = y - ≈∑ |
| **Positive residual** | Observed above line |
| **Negative residual** | Observed below line |

### Properties of Residuals

| Property | Explanation |
|----------|-------------|
| **Sum to zero** | Œ£(residuals) = 0 |
| **Mean = 0** | Always |
| **Measure fit** | Smaller = better fit |

### Residual Plots

| Purpose | Description |
|---------|-------------|
| **Check linearity** | Random scatter = good |
| **X-axis** | x values (or fitted values) |
| **Y-axis** | Residuals |

### Patterns in Residual Plots

| Pattern | Interpretation |
|---------|----------------|
| **Random scatter** | Linear model appropriate |
| **Curved pattern** | Nonlinear relationship |
| **Fan shape** | Changing spread |
| **Outliers** | Individual unusual points |

### Residual Plot Examples

**Good Residual Plot (Random Scatter):**
```
    +  |     ‚Ä¢    ‚Ä¢
       |  ‚Ä¢    ‚Ä¢      ‚Ä¢
 0  ---|----‚Ä¢----‚Ä¢----‚Ä¢---
       |     ‚Ä¢  ‚Ä¢   ‚Ä¢
    -  | ‚Ä¢       ‚Ä¢
       +-----------------‚Üí x
```
Linear model is appropriate.

**Bad Residual Plot (Curved Pattern):**
```
    +  |           ‚Ä¢  ‚Ä¢
       |        ‚Ä¢
 0  ---|--‚Ä¢--‚Ä¢--------‚Ä¢--
       |   ‚Ä¢  ‚Ä¢
    -  | ‚Ä¢
       +-----------------‚Üí x
```
Relationship is not linear - consider transformation.

**Bad Residual Plot (Fan Shape):**
```
    +  |           ‚Ä¢
       |     ‚Ä¢   ‚Ä¢  ‚Ä¢
 0  ---|--‚Ä¢--‚Ä¢-------‚Ä¢---
       |     ‚Ä¢   ‚Ä¢  ‚Ä¢
    -  |           ‚Ä¢
       +-----------------‚Üí x
```
Variance is not constant - indicates heteroscedasticity.

---

## üìà Coefficient of Determination (r¬≤)

### Definition

| Concept | Description |
|---------|-------------|
| **r¬≤** | Proportion of variation in y explained by x |
| **Range** | 0 ‚â§ r¬≤ ‚â§ 1 |
| **Calculation** | r¬≤ = (r)¬≤ |

### Interpreting r¬≤

| Template | |
|----------|--|
| **Say** | "[r¬≤ √ó 100]% of the variation in [y] is explained by the linear relationship with [x]." |
| **Example** | "64% of the variation in test scores is explained by the linear relationship with study hours." |

### What r¬≤ Tells You

| r¬≤ Value | Interpretation |
|----------|----------------|
| **r¬≤ = 0.81** | 81% of y variation explained |
| **1 - r¬≤ = 0.19** | 19% unexplained (residual) |
| **Higher = better** | Model explains more |

---

## ‚ö†Ô∏è Cautions in Regression

### Extrapolation

| Concept | Description |
|---------|-------------|
| **Definition** | Predicting outside data range |
| **Problem** | Pattern may not continue |
| **Dangerous** | Often leads to bad predictions |

### Interpolation

| Concept | Description |
|---------|-------------|
| **Definition** | Predicting within data range |
| **Safer** | Pattern observed in this range |

### Outliers and Influential Points

| Term | Definition |
|------|------------|
| **Outlier** | Point far from regression line |
| **High leverage** | Point with extreme x value |
| **Influential** | Removing changes line significantly |

### Influential Point Effects

| If | Effect |
|----|--------|
| **Point has high leverage** | May be influential |
| **Influential point present** | Report with and without |

### Types of Unusual Points

| Type | Definition | Example |
|------|------------|---------|
| **Outlier** | Point far from the regression line (large residual) | Student who studied 5 hours but scored only 50 |
| **High Leverage** | Point with extreme x-value | Student who studied 15 hours when others studied 2-6 |
| **Influential** | Point that significantly changes the regression line when removed | High-leverage point that pulls the line |

### Identifying Influential Points

| Check | Method |
|-------|--------|
| Calculate regression with point | Note slope, intercept, r |
| Calculate regression without point | Compare slope, intercept, r |
| If substantial change | Point is influential |

### Example: Influential Point Analysis

**Original LSRL:** ≈∑ = 10 + 2.5x, r¬≤ = 0.85

**Remove suspected influential point:**
**New LSRL:** ≈∑ = 15 + 1.8x, r¬≤ = 0.72

The slope changed from 2.5 to 1.8 and r¬≤ dropped from 0.85 to 0.72. This point is influential!

---

## üîÑ Lurking Variables and Causation

### Lurking Variable

| Concept | Description |
|---------|-------------|
| **Definition** | Variable not in study that affects both x and y |
| **Effect** | Can create false correlation |
| **Example** | Ice cream sales and drowning (lurking: temperature) |

### Confounding Variable

| Concept | Description |
|---------|-------------|
| **Definition** | Variable whose effect can't be separated from x |
| **Problem** | Can't determine which causes y |

### Classic Examples of Lurking/Confounding Variables

| Observed Correlation | Lurking Variable |
|---------------------|------------------|
| Ice cream sales ‚Üî Drowning deaths | Temperature (summer) |
| Shoe size ‚Üî Reading ability | Age of child |
| Number of firefighters ‚Üî Fire damage | Size of fire |
| Coffee drinking ‚Üî Heart disease | Smoking habits |
| TV ownership ‚Üî Life expectancy | Country's wealth |

### Why Correlation ‚â† Causation

| Issue | Explanation |
|-------|-------------|
| **Lurking variables** | Third variable causes both |
| **Reverse causation** | y might cause x instead |
| **Coincidence** | Random chance, no real connection |
| **Common response** | Both respond to same cause |

### Establishing Causation

| Method | Description |
|--------|-------------|
| **Randomized experiment** | Gold standard |
| **Control** | Lurking variables |
| **Temporal order** | Cause before effect |
| **Mechanism** | Logical explanation |

---

## üìö Practice Problems with Solutions

### Practice Problem 1: Interpret Slope

**Problem:** A regression equation relating hours of sleep (x) to GPA (y) is: ≈∑ = 1.5 + 0.25x

Interpret the slope in context.

**Solution:** For each additional hour of sleep per night, we predict GPA to increase by 0.25 points.

### Practice Problem 2: Calculate Residual

**Problem:** Using the equation ≈∑ = 10 + 3x, find the residual for the point (5, 28).

**Solution:**
- Predicted value: ≈∑ = 10 + 3(5) = 25
- Residual = Observed - Predicted = 28 - 25 = 3
- The actual value is 3 units above the predicted value.

### Practice Problem 3: Interpret r¬≤

**Problem:** The correlation between study time and exam score is r = 0.8. Interpret r¬≤.

**Solution:**
- r¬≤ = (0.8)¬≤ = 0.64
- Interpretation: 64% of the variation in exam scores is explained by the linear relationship with study time.
- The remaining 36% is due to other factors.

### Practice Problem 4: Identify Extrapolation

**Problem:** Data was collected on cars with ages 2-10 years. The regression equation is: ≈∑ = 25000 - 2000x (where y = price, x = age). Predict the price for a 1-year-old car and a 15-year-old car.

**Solution:**
- 1-year-old: ≈∑ = 25000 - 2000(1) = $23,000 (extrapolation - outside data range)
- 15-year-old: ≈∑ = 25000 - 2000(15) = -$5,000 (extrapolation - impossible value!)

The 15-year prediction shows why extrapolation is dangerous - the linear pattern doesn't continue.

### Practice Problem 5: LSRL Calculation

**Problem:** Given: xÃÑ = 10, »≥ = 50, s‚Çì = 2, s·µß = 8, r = 0.75. Find the LSRL.

**Solution:**
- Slope: b = r(s·µß/s‚Çì) = 0.75(8/2) = 3
- Intercept: a = »≥ - bxÃÑ = 50 - 3(10) = 20
- LSRL: ≈∑ = 20 + 3x

---

### Correlation Does NOT Imply Causation

| Can Prove | Cannot Prove |
|-----------|--------------|
| **Association** | Causation |
| **Relationship** | Direction of cause |
| **Prediction** | Mechanism |

### Establishing Causation

| Method | Description |
|--------|-------------|
| **Randomized experiment** | Gold standard |
| **Control** | Lurking variables |
| **Temporal order** | Cause before effect |
| **Mechanism** | Logical explanation |

---

## üìä Transforming Data

### Why Transform?

| Reason | Description |
|--------|-------------|
| **Curved relationship** | Make it linear |
| **Fan-shaped residuals** | Stabilize variance |

### Common Transformations

| Transformation | When to Use |
|----------------|-------------|
| **log(y)** | Exponential growth |
| **log(x)** | Power relationship |
| **log(x) and log(y)** | Power model |
| **‚àöy** | Variance stabilization |

### Exponential Model

| Original | y = ab^x |
|----------|----------|
| **Transform** | log(y) = log(a) + x¬∑log(b) |
| **Plot** | x vs. log(y) should be linear |

### Power Model

| Original | y = ax^b |
|----------|----------|
| **Transform** | log(y) = log(a) + b¬∑log(x) |
| **Plot** | log(x) vs. log(y) should be linear |

---

## üìê Regression Inference (Preview)

### Regression Model

| Population | Sample |
|------------|--------|
| **Œº·µß = Œ± + Œ≤x** | ≈∑ = a + bx |
| **Œ≤ (population slope)** | b (sample slope) |
| **Œ± (population intercept)** | a (sample intercept) |

### Standard Error of Slope

| Symbol | s·µ¶ |
|--------|-----|
| **Measures** | Variability in slope estimate |
| **Used for** | Confidence intervals and tests |

---

## üìä Comparing Regression Models

### Which Model is Better?

| Criterion | Description |
|-----------|-------------|
| **Higher r¬≤** | Explains more variation |
| **Random residuals** | No pattern |
| **Smaller s** | Less prediction error |
| **Makes sense** | Context appropriate |

### Standard Deviation of Residuals (s)

| Concept | Description |
|---------|-------------|
| **Symbol** | s |
| **Measures** | Typical residual size |
| **Smaller = better** | Predictions closer to actual |

---

## üìù Key Terms Glossary

| Term | Definition |
|------|------------|
| **Scatterplot** | Graph of paired (x, y) data |
| **Explanatory variable** | x, independent |
| **Response variable** | y, dependent |
| **Correlation (r)** | Linear strength and direction |
| **LSRL** | Best-fit line |
| **Residual** | Observed - Predicted |
| **r¬≤** | Proportion of variation explained |
| **Extrapolation** | Predicting outside data range |
| **Lurking variable** | Hidden variable affecting both |
| **Influential point** | Point that changes regression |
| **High leverage** | Extreme x value |

---

## üéØ AP Exam Strategies

### Free Response Tips

| Task | How to Answer |
|------|---------------|
| **Describe scatterplot** | DOFS (direction, outliers, form, strength) |
| **Interpret slope** | "For each additional [x], [y] changes by [b]" |
| **Interpret r¬≤** | "[r¬≤]% of variation in [y] explained by [x]" |
| **Residual plot** | Describe pattern (or lack thereof) |

### Common Mistakes

| Mistake | Correction |
|---------|------------|
| **Correlation = causation** | Never conclude causation from r |
| **Extrapolate** | Only interpolate |
| **Forget context** | Name both variables |
| **Ignore residual plot** | Always check |

### Calculator Steps (TI-84)

| Task | Steps |
|------|-------|
| **Enter data** | STAT ‚Üí Edit |
| **Regression** | STAT ‚Üí CALC ‚Üí LinReg(a+bx) |
| **Scatterplot** | 2nd ‚Üí STATPLOT |
| **Residual list** | RESID in NAMES menu |

---

**Good luck on your AP Statistics exam! üçÄüìàüìä**

Remember: Correlation measures LINEAR relationships only. Always check residual plots before trusting your regression!
